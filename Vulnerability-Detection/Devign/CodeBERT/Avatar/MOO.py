import csv
import time
import random
import logging
import numpy as np

from distill_utils import distill_surrogate
from tqdm import tqdm
from surrogate import predictor
from flops import TransformerHparams
from reduction import Reduction_z3

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
                    datefmt="%m/%d/%Y %H:%M:%S",
                    level=logging.INFO)

class Candidate(object):
    def __init__(self, candidates_vals):
        self.candidate_values = candidates_vals
        self.objective_values = []

    def get_candidate_values(self):
        return self.candidate_values

    def set_objective_values(self, objective_values):
        self.objective_values = objective_values

    def get_objective_values(self):
        return self.objective_values

    def set_candidate_values_at_index(self, indx, val):
        self.candidate_values[indx] = val

def generate_random_population(size, lb, ub):
    random_pop = []

    for i in range(size):
        while True:
            candidate_vals = []
            for index in range(len(lb)):
                candidate_vals.append(int(random.uniform(lb[index], ub[index])))
            if candidate_vals[0] % candidate_vals[-1] == 0:
                break
        random_pop.append(Candidate(candidate_vals))

    return random_pop

def calculate_minimum_distance(candidate, random_pop):
    distance = 1e9

    for each_candidate in random_pop:
        vals = each_candidate.get_candidate_values()
        candidate_vals = candidate.get_candidate_values()
        dist = np.linalg.norm(np.array(vals) - np.array(candidate_vals))
        if dist < distance:
            distance = dist

    return distance

def generate_adaptive_random_population(size, lb, ub):
    random_pop = []

    initial_pop = generate_random_population(10, lb, ub)[0]

    random_pop.append(initial_pop)

    while len(random_pop) < size:
        D = 0
        selected_candidate = None
        rp = generate_random_population(size, lb, ub)
        for each_candidate in rp:
            min_dis = calculate_minimum_distance(each_candidate, random_pop)
            if min_dis > D:
                D = min_dis
                selected_candidate = each_candidate

        random_pop.append(selected_candidate)

    return random_pop

def evaulate_population(population, surrogate_model):
    for each_candidate in population:
        candidate_values = each_candidate.get_candidate_values()
        model = TransformerHparams(candidate_values[0], candidate_values[1], 514, candidate_values[2], candidate_values[3], candidate_values[4])
        size = model.get_params()*4/1e6
        flops = model.get_infer_flops()/1e9
        accuracy = surrogate_model.predict([[candidate_values[0], candidate_values[1], candidate_values[2], candidate_values[3], candidate_values[4]]])[0]
        fitnesses = [size, accuracy, flops]
        each_candidate.set_objective_values(fitnesses)

def dominates(candidate1, candidate2):
    candidate1_objectives = candidate1.get_objective_values()
    candidate2_objectives = candidate2.get_objective_values()
    dominates = False
    if candidate1_objectives[0] < candidate2_objectives[0] and candidate1_objectives[1] > candidate2_objectives[1] and candidate1_objectives[2] < candidate2_objectives[2]:
        dominates = True
    return dominates

def update_archive(pop, archive):
    for each_candidate in pop:
        dominated = False
        for each_archive in archive:
            if dominates(each_archive, each_candidate):
                dominated = True
                break
        if not dominated:
            if len(archive) == 0:
                archive.append(each_candidate)
            else:
                to_remove = []
                for each_archive in archive:
                    if dominates(each_candidate, each_archive) or each_archive.get_candidate_values() == each_candidate.get_candidate_values():
                        to_remove.append(each_archive)

                for each_remove in to_remove:
                    archive.remove(each_remove)
                archive.append(each_candidate)

def simulated_binary_crossover(parent1, parent2, nc=10):
    parent1 = parent1.get_candidate_values()
    parent2 = parent2.get_candidate_values()
    u = random.uniform(0, 1)
    if u < 0.5:
        B = (2 * u) ** (1 / (nc + 1))
    else:
        B = (1 / (2 * (1 - u))) ** (1 / (nc + 1))
    t_parent1 = []
    t_parent2 = []

    for indx in range(len(parent1)):
        x1 = parent1[indx]
        x2 = parent2[indx]
        x1new = 0.5 * (((1 + B) * x1) + ((1 - B) * x2))
        x2new = 0.5 * (((1 - B) * x1) + ((1 + B) * x2))
        t_parent1.append(x1new)
        t_parent2.append(x2new)

    return Candidate(t_parent1), Candidate(t_parent2)

def gaussain_mutation(candidate, lb, ub, thresh):
    candidate_vals = candidate.get_candidate_values()

    for attrib in range(len(candidate_vals)):
        if random.uniform(0, 1) > thresh:
            continue
        mu = 0
        sigma = 1
        alpha = np.random.normal(mu, sigma)
        actualValueP1 = candidate_vals[attrib];

        if (alpha < 1) and (alpha >= 0):
            if actualValueP1 + 1 < ub[attrib]:
                candidate_vals[attrib] = candidate_vals[attrib] + 1
        elif (alpha <= 0) and (alpha > -1):
            if actualValueP1 - 1 > lb[attrib]:
                candidate_vals[attrib] = candidate_vals[attrib] - 1
        else:
            if actualValueP1 + alpha < ub[attrib]:
                candidate_vals[attrib] = candidate_vals[attrib] + alpha
    return Candidate(candidate_vals)


def correct(pop, lb, ub):
    for indx in range(len(pop)):
        candidate = pop[indx]
        values = candidate.get_candidate_values()
        for value_index in range(len(values)):
            pop[indx].set_candidate_values_at_index(value_index, int(pop[indx].get_candidate_values()[value_index]))
            while values[value_index] > ub[value_index] or values[value_index] < lb[value_index]:
                temp = generate_random_population(1, lb, ub)[0]
                pop[indx].set_candidate_values_at_index(value_index, int(temp.get_candidate_values()[value_index]))

        while values[0] % values[-1] != 0:
            temp = generate_random_population(1, lb, ub)[0]
            pop[indx].set_candidate_values_at_index(0, int(temp.get_candidate_values()[0]))

    return pop

def select_best(tournament_candidates):
    best = tournament_candidates[0]
    for i in range(len(tournament_candidates)):
        candidate1 = tournament_candidates[i]
        for j in range(len(tournament_candidates)):
            candidate2 = tournament_candidates[j]
            if (dominates(candidate1, candidate2)):
                best = candidate1
    return best

def tournament_selection(pop, size):
    tournament_candidates = []
    for i in range(size):
        indx = random.randint(0, len(pop) - 1)
        random_candidate = pop[indx]
        tournament_candidates.append(random_candidate)

    best = select_best(tournament_candidates)
    return best

def generate_off_springs(pop, lb, ub):
    size = len(pop)
    population_to_return = []

    while len(population_to_return) < size:
        parent1 = tournament_selection(pop, 10)
        parent2 = tournament_selection(pop, 10)
        while parent1 == parent2:
            parent2 = tournament_selection(pop, 10)
        probability_crossover = random.uniform(0, 1)
        if probability_crossover <= 0.60:
            parent1, parent2 = simulated_binary_crossover(parent1, parent2)
        child1 = gaussain_mutation(parent1, lb, ub, (1 / len(parent1.get_candidate_values())))
        child2 = gaussain_mutation(parent2, lb, ub, (1 / len(parent1.get_candidate_values())))

        population_to_return.append(child1)
        population_to_return.append(child2)

    return population_to_return

if __name__ == "__main__":
    start_time = time.time()
    search_space = Reduction_z3()
    lb = search_space.get_lower_bounds()
    ub = search_space.get_upper_bounds()
    # lb = [16, 1, 100, 16, 1]
    # ub = [768, 12, 50000, 3072, 12]

    pop = generate_adaptive_random_population(20, lb, ub)

    surrogate_data = []
    for each_pop in pop:
        surrogate_data.append(each_pop.get_candidate_values())

    surrogate_data_acc = distill_surrogate(surrogate_data)
    surrogate_model = predictor([surrogate_data, surrogate_data_acc])

    # import ast
    # with open("accs.jsonl") as f:
    #     data = f.readlines()
    #     surrogate_data_acc = []
    #     data1 = []
    #     data2 = []
    #     for line in data:
    #         data1.append(ast.literal_eval(line.split("] ")[0]+"]"))
    #         data2.append(float(line.split("] ")[1]))

    # surrogate_data_acc = [data1, data2]
    # surrogate_model = predictor(surrogate_data_acc)

    evaulate_population(pop, surrogate_model)
    archive = []
    update_archive(pop, archive)

    iteration = 100
    for i in tqdm(range(iteration)):
        pop = generate_off_springs(pop, lb, ub)
        pop = correct(pop, lb, ub)
        evaulate_population(pop, surrogate_model)
        update_archive(pop, archive)

    logging.info("Time taken: {}".format(time.time() - start_time))
    logging.info("Number of solutions in the archive: {}".format(len(archive)))
    logging.info("Saving the archive to the file")
    with open("pareto_set.csv", "w") as f:
        writer = csv.writer(f)
        writer.writerow(["H", "L", "V", "I", "A", "Size", "Accuracy", "FLOPS"])
        for each_archive in archive:
            writer.writerow(each_archive.get_candidate_values() + each_archive.get_objective_values())