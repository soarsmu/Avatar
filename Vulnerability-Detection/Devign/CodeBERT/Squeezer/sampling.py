import copy
import json
import random
import logging
import hashlib
import warnings
import argparse

from tqdm import tqdm
from flops import TransformerHparams

from transformers import RobertaConfig, RobertaModel
warnings.filterwarnings("ignore")

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO
)

logger = logging.getLogger(__name__)

from surrogate import Kriging
from RBF import Model

data = []
with open("surrogate_acc.jsonl") as f:
    for line in f:
        data.append(json.loads(line.strip()))

cluster = []

for d in data:
    temp = []
    for c in d.values():
        temp.append(c)
    cluster.append(temp)

pre = Model(10,cluster)

pre.test(cluster)

# print(pre.predict([45000, 8, 16, 496, 1])[0][0])
# exit()

class Genome(object):
    def __init__(self, gene_param=None):
        self.fitness = 0.0
        self.gene_param = gene_param

        if not self.gene_param:
            self.hash = 0
        else:
            self.update_hash()
    
    def update_hash(self):
        gene_string = str(self.gene_param["intermediate_size"])+ \
                        str(self.gene_param["vocab_size"]) + \
                        str(self.gene_param["attention_heads"]) + \
                        str(self.gene_param["hidden_dim"]) + \
                        str(self.gene_param["n_layers"]) 
        self.hash = hashlib.md5(gene_string.encode("UTF-8")).hexdigest()

    def mutation(self, search_space):
        genome_len = len(self.gene_param)
        loc = random.randint(0, genome_len - 1)

        for x in range(genome_len):
            if x >= loc:
                mutated_gene = list(self.gene_param.keys())[x]
                current_value = self.gene_param[mutated_gene]
                possible_choices = copy.deepcopy(search_space[mutated_gene])
                possible_choices.remove(current_value)
                self.gene_param[mutated_gene] = random.choice(possible_choices)
        self.update_hash()


class GA_search():
    def __init__(self, args, search_space, cross_chance=0.6):
        self.args = args
        self.search_space = search_space
        self.cross_chance = cross_chance
        self.desired_length = args.population_size
        self.population = []
        self.best_gene = []

    def is_duplicate(self, new_genome):
        for genome in self.population:
            if new_genome.hash == genome.hash:
                return True
        return False
    
    def is_uncorrect(self, genome):
        if genome.gene_param["hidden_dim"] % genome.gene_param["attention_heads"]==0:
        # config = RobertaConfig.from_pretrained("microsoft/codebert-base")
        # config.num_attention_heads = 12
        # config.hidden_size = 84
        # config.intermediate_size = 303
        # config.vocab_size = genome.gene_param["vocab_size"]
        # config.num_hidden_layers = genome.gene_param["n_layers"]
        # model = RobertaModel(config=config)
        # del model
            return False
        return True
        # except:
        #     print(1)
        #     return True

    def initialization(self):
        count = 0

        while count < self.args.population_size:
            gene_param = {}
            for key in self.search_space:
                gene_param[key] = random.choice(self.search_space[key])
            new_genome = Genome(gene_param)
            
            if len(self.population) > 0:
                while self.is_duplicate(new_genome) or self.is_uncorrect(new_genome):
                    new_genome.mutation(self.search_space)

            self.population.append(copy.deepcopy(new_genome))
            count += 1
    
    def fitness(self, genome):
        vocab_size = genome.gene_param["vocab_size"]
        attention_heads = genome.gene_param["attention_heads"]
        hidden_dim = genome.gene_param["hidden_dim"]
        intermediate_size = genome.gene_param["intermediate_size"]
        n_layers = genome.gene_param["n_layers"]
        model = TransformerHparams(hidden_dim, n_layers, 514, vocab_size, intermediate_size, attention_heads)
        flops = model.get_infer_flops()
        params = model.get_params()
        
        size_diff = abs(self.args.target_size - params)*4/1e6
        genome.fitness = pre.predict([vocab_size, attention_heads, hidden_dim, intermediate_size, n_layers]) - flops/1e9 - size_diff

    def crossover_and_mutation(self, parents):
        children = []
        parent_1, parent_2 = parents
        if self.cross_chance > random.random():
            genome_len = len(self.search_space)
            recomb_loc = random.randint(1, genome_len - 1)

            child_1 = {}
            child_2 = {}

            keys = list(self.search_space)
            keys = sorted(keys)

            for x in range(0, genome_len):
                if x < recomb_loc:
                    child_1[keys[x]] = parent_1.gene_param[keys[x]]
                    child_2[keys[x]] = parent_2.gene_param[keys[x]]
                else:
                    child_1[keys[x]] = parent_2.gene_param[keys[x]]
                    child_2[keys[x]] = parent_1.gene_param[keys[x]]
            genome_1 = Genome(child_1)
            genome_2 = Genome(child_2)
        else:
            genome_1 = copy.deepcopy(parent_1)
            genome_2 = copy.deepcopy(parent_2)

            genome_1.mutation(self.search_space)
            genome_2.mutation(self.search_space)

        children.append(genome_1)
        children.append(genome_2)

        return children

    def generation(self):
        children = []
        while len(children) < self.desired_length:
            parents = random.sample(self.population, k=2)
            children.extend(self.crossover_and_mutation(parents))

        for genome in children:
            while self.is_duplicate(genome) or self.is_uncorrect(genome):
                genome.mutation(self.search_space)
            self.population.append(copy.deepcopy(genome))
        
        for genome in self.population:
            self.fitness(genome)

        graded_genome = [x for x in sorted(self.population, key=lambda x: x.fitness, reverse=True)]
        self.best_gene.append((graded_genome[0].gene_param, graded_genome[0].fitness))
        self.population = graded_genome[:self.args.population_size]

import time
def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--population_size", default=50, type=int)
    parser.add_argument("--generation_size", default=100, type=int)
    parser.add_argument("-t", "--target_size", default=3, type=float)

    args = parser.parse_args()
    search_space = {
        "vocab_size": [*range(1000, 51000)],
        "attention_heads": [*range(1, 13)],
        "hidden_dim": [*range(16, 769)],
        "intermediate_size": [*range(16, 3072)],
        "n_layers": [*range(1, 13)]
    }

    args.target_size = args.target_size * 1e6/4
    logger.info("***Start GA search for %d generations, %d population, target model size %d MB***" %
          (args.generation_size, args.population_size, args.target_size*4/1e6))

    searcher = GA_search(args, search_space)
    start = time.time()
    searcher.initialization()
    for gen in tqdm(range(args.generation_size), desc="Searching"):
        # logger.info("***Start generate %d***" %(gen))
        searcher.generation()

    print(time.time() - start)
    for genome in searcher.population:
        searcher.fitness(genome)
    graded_genome = [x for x in sorted(searcher.population, key=lambda x: x.fitness, reverse=True)]

    logger.info("the best one:")
    logger.info(graded_genome[0].gene_param)
    with open("best_genes.jsonl", "w") as f:
        for g in searcher.best_gene:
            f.write(str(g[1]))
            f.write("\n")


if __name__ == "__main__":
    main()
