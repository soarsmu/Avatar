import json
import numpy as np

data = []
with open("unlabel_train.jsonl") as f:
    for line in f:
        data.append(json.loads(line.strip()))

preds = np.load("preds_unlabel_train_gcb.npy").tolist()
# print(preds)
new_data = []
for d, p in zip(data, preds):
    d["soft_label"] = p
    new_data.append(d)

with open("soft_unlabel_train_gcb.jsonl", "w") as f:
    for d in new_data:
        f.write(json.dumps(d) + "\n")

preds = np.load("preds_unlabel_train_gcb.npy").tolist()

print(len(preds))

CUDA_VISIBLE_DEVICES=1 python3 distill.py \
    --do_train \
    --train_data_file=../../data/soft_unlabel_train.jsonl \
    --eval_data_file=../../data/test.jsonl \
    --size 3 \
    --attention_heads 16 \
    --hidden_dim 96 \
    --intermediate_size 64 \
    --n_layers 12 \
    --vocab_size 1000 \
    --block_size 400 \
    --train_batch_size 16 \
    --eval_batch_size 100 \
    --learning_rate 1e-4 \
    --epochs 30 \
    --seed 123456

CUDA_VISIBLE_DEVICES=3 python3 distill.py \
    --do_train \
    --train_data_file=../data/soft_unlabel_train.jsonl \
    --eval_data_file=../data/test.jsonl \
    --size 3 \
    --attention_heads 16 \
    --hidden_dim 96 \
    --intermediate_size 64 \
    --n_layers 12 \
    --vocab_size 1000 \
    --block_size 400 \
    --train_batch_size 16 \
    --eval_batch_size 64 \
    --learning_rate 1e-4 \
    --epochs 30 \
    --seed 123456